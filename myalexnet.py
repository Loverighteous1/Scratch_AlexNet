# -*- coding: utf-8 -*-
"""MyAlexNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AqkAZmhudCuxJip-h77sOmGccy5TYeL1

# Implementation based on ImageNet Classification with Deep CNN paper of Alex et al. on CIFAR10 and CIFAR100

### Needful libraries
"""

import torch as t
import torch.nn as nn
import torch.optim as optim # it provides SGD and Adam
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision import datasets
#the DataLoader is a versatile tool in PyTorch that streamlines the data loading process,
#supports efficient batch processing, facilitates data augmentation and normalization, and enables parallelism, contributing to smoother and faster model training.

datasets.CIFAR100

"""### Data loading, preparation, and augmentation"""

import tarfile
import os

# Define the path to the tarball file
tar_path = '/content/drive/MyDrive/cifar-100-python.tar.gz'

# Define the extraction path
extract_path = '/content/cifar-100-python'

# Create the extraction directory if it doesn't exist
os.makedirs(extract_path, exist_ok=True)

# Open and extract the tarball file
with tarfile.open(tar_path, 'r:gz') as tar:
    tar.extractall(path=extract_path)

print("Extraction completed.")

import os

# List the contents of the extraction path
extracted_files = os.listdir(extract_path)
print("Extracted files:", extracted_files)

# Define the path to the tarball file
tar_path = '/content/drive/MyDrive/cifar-100-python.tar.gz'

# Define the extraction path
extract_path = '/content/drive/MyDrive/Computer Vision/cifar-100-python'

# Create the extraction directory if it doesn't exist
os.makedirs(extract_path, exist_ok=True)

# Open and extract the tarball file
with tarfile.open(tar_path, 'r:gz') as tar:
    tar.extractall(path=extract_path)

print("Extraction completed.")
#import os

# List the contents of the extraction path
extracted_files = os.listdir(extract_path)
print("Extracted files:", extracted_files)

ls

# Define the path to the tarball file
tar_path = '/content/drive/MyDrive/cifar-10-python.tar.gz'

# Define the extraction path
extract_path = '/content/drive/MyDrive/Computer Vision/cifar-10-python'

# Create the extraction directory if it doesn't exist
os.makedirs(extract_path, exist_ok=True)

# Open and extract the tarball file
with tarfile.open(tar_path, 'r:gz') as tar:
    tar.extractall(path=extract_path)

print("Extraction completed.")
#import os

# List the contents of the extraction path
extracted_files = os.listdir(extract_path)
print("Extracted files:", extracted_files)

!unzip "/content/drive/MyDrive/cifar-10-python.tar.gz"

# Define data augmentation transformations
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load CIFAR-10 and CIFAR-100 datasets with data augmentation
# The data augmentation is performed only during the training process. That is why the transform components
# differ from training tranform to test transform
cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
cifar10_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)

cifar100_train = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)
cifar100_test = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)

# Create data loaders
train_loader = DataLoader(cifar10_train, batch_size=64, shuffle=True)

"""### Model developpment"""

#Parameters initilization
"""
The first two initialisation are done within a function
1- Weight Initialization: The weights in the neural network were initialized using a Gaussian distribution
 with mean 0 and standard deviation 0.01.

2- Bias Initialization: The biases in the neural network were initialized to a constant value of 0.
 ******************************************************************************************************


3- Learning Rate: The initial learning rate used in training the network was set to 0.01.

4- Momentum: The momentum parameter used in stochastic gradient descent was set to 0.9.

5- Weight Decay: A weight decay parameter of 0.0005 was used to prevent overfitting during training.

6- Batch Size: The batch size used for training the network was set to 128.

7- Dropout Probability: In the fully connected layers of the network, dropout with a probability of 0.5 was applied during training to prevent overfitting.

8- Image Size: The input images were resized to 256x256 pixels before being cropped to 224x224 pixels for training.
 """
#hyperparameters
learning_rate = 0.01
momentum = 0.9
weight_decay = 0.0005
batchsiz = 128
img_siz = 256

#Local response normalisation hyperparameters determined by the authors using the validation set
N = 5 #N is the number of kernels in the layer
alfa = 10**(-4)
bta = 0.75
K = 2 #additive factor


 ##Others parameters
clas = 10
epochs = 5

#Alex Model Construction
class AlexNet(nn.Module):
  def__init_(self, num_classes):
    super(AlexNet, self).__init__()

  self.network = nn.Sequential(
            #first block conv layer1
            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2),  # (b x 96 x 55 x 55)
            nn.ReLU(inplace=True),
            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),
            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 96 x 27 x 27)

            #second block
            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),  # (b x 256 x 27 x 27)
            nn.ReLU(inplace=True),
            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),
            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 13 x 13)

            #third block
            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1,padding=1),  # (b x 384 x 13 x 13)
            nn.ReLU(inplace=True),

            #fourth block
            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),  # (b x 384 x 13 x 13)
            nn.ReLU(inplace=True),

            #fifth block
            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),  # (b x 256 x 13 x 13)
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 6 x 6)

            #convert the 4D output into 2D tensor
            nn.flatten(),
            # Full connnections block
            nn.Dropout(p=0.5, inplace=True),
            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5, inplace=True),
            nn.Linear(in_features=4096, out_features=4096),
            nn.ReLU(inplace=True),
            )

  #call the function init weight and biases
  init_weight_bias()

  def init_weight_bias():
        for layer in network:
            if isinstance(layer, nn.Conv2d):
                nn.init.normal_(layer.weight, mean=0, std=0.01) # Gaussian intialization
                nn.init.constant_(layer.bias, 0)
        # original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers
        # the process is index-based so starting from layer o, 1st conv we have the below matching
        nn.init.constant_(network[4].bias, 1) # 2nd conv layer
        nn.init.constant_(network[10].bias, 1) #4th conv layer
        nn.init.constant_(network[12].bias, 1) #5th conv layer

  def forward():
    return

"""### Training"""

#Optimization block
criterion = nn.CrossEntropyLoss()
#optimizer used : stochastic gradient descent with momentum
alex_optimizer = optim.SGD(network.parameters(), lr = learning_rate, momentum = momentum, weight_decay=weight_decay)

# Training loop
def train(network, train_loader, criterion, optimizer, num_epochs=epochs):
    network.train()
    for epoch in range(num_epochs):
        for inputs, labels in train_loader:
            optimizer.zero_grad()   # clear the previous step
            outputs = network(inputs)
            loss = criterion(outputs, labels)
            loss.backward() # compute gradients
            optimizer.step() #uses these gradients to adjust the parameters in a direction that reduces the loss.
        print(f'Epoch {epoch+1}, Loss: {loss.item()}')

#train the model
train(network, train_loader, criterion, alex_optimizer)

"""### Evaluation"""

# Evaluation function
def evaluate(network, test_loader):
    network.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = network(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    print(f'Accuracy on the test set: {accuracy}%')

# Evaluate the model
test_loader = DataLoader(cifar10_test, batch_size=64, shuffle=False)
evaluate(network, test_loader)